{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Demand Forecasting Project - 改进版\n",
    "## Data Science Internship Project\n",
    "\n",
    "### 项目流程\n",
    "\n",
    "**第一周： 数据处理**\n",
    "1. 数据加载\n",
    "2. 探索性数据分析（EDA）\n",
    "3. 离群值检测和清理\n",
    "4. 数据预处理\n",
    "5. 特征工程\n",
    "6. 数据拆分和验证\n",
    "\n",
    "**第二周： 模型训练**\n",
    "1. 模型训练\n",
    "2. 模型评估\n",
    "3. 模型调参\n",
    "4. 模型预测\n",
    "5. 模型保存和加载\n",
    "6. 模型部署"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test data\n",
    "train_data = pd.read_csv('../archive/train_0irEZ2H.csv')\n",
    "test_data = pd.read_csv('../archive/test_nfaJ3J5.csv')\n",
    "\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "print(\"\\nTraining data preview:\")\n",
    "display(train_data.head())\n",
    "\n",
    "print(\"\\nTest data preview:\")\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and basic info\n",
    "print(\"Training data info:\")\n",
    "train_data.info()\n",
    "\n",
    "print(\"\\nTest data info:\")\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. 探索性数据分析（EDA）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data description:\")\n",
    "display(train_data.describe())\n",
    "\n",
    "print(\"Test data description:\")\n",
    "display(test_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "missing_train = train_data.isnull().sum()\n",
    "print(missing_train[missing_train > 0])\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "missing_test = test_data.isnull().sum()\n",
    "print(missing_test[missing_test > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert week column to datetime\n",
    "train_data['week'] = pd.to_datetime(train_data['week'], format='%y/%m/%d')\n",
    "test_data['week'] = pd.to_datetime(test_data['week'], format='%y/%m/%d')\n",
    "\n",
    "print(\"Date range in training data:\")\n",
    "print(f\"Start: {train_data['week'].min()}, End: {train_data['week'].max()}\")\n",
    "\n",
    "print(\"\\nDate range in test data:\")\n",
    "print(f\"Start: {test_data['week'].min()}, End: {test_data['week'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable (units_sold)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_data['units_sold'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Units Sold')\n",
    "plt.xlabel('Units Sold')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(train_data['units_sold'])\n",
    "plt.title('Boxplot of Units Sold')\n",
    "plt.ylabel('Units Sold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Statistics for units_sold:\")\n",
    "print(f\"Mean: {train_data['units_sold'].mean():.2f}\")\n",
    "print(f\"Median: {train_data['units_sold'].median():.2f}\")\n",
    "print(f\"Std: {train_data['units_sold'].std():.2f}\")\n",
    "print(f\"Min: {train_data['units_sold'].min()}\")\n",
    "print(f\"Max: {train_data['units_sold'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price analysis\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train_data['total_price'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Total Price')\n",
    "plt.xlabel('Total Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(train_data['base_price'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Base Price')\n",
    "plt.xlabel('Base Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "price_diff = train_data['base_price'] - train_data['total_price']\n",
    "plt.hist(price_diff, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Price Difference')\n",
    "plt.xlabel('Base Price - Total Price')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Store analysis\n",
    "store_counts = train_data['store_id'].value_counts()\n",
    "axes[0, 0].bar(store_counts.index, store_counts.values)\n",
    "axes[0, 0].set_title('Number of Records per Store')\n",
    "axes[0, 0].set_xlabel('Store ID')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "\n",
    "# SKU analysis\n",
    "sku_counts = train_data['sku_id'].value_counts()\n",
    "axes[0, 1].hist(sku_counts.values, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution of Records per SKU')\n",
    "axes[0, 1].set_xlabel('Number of Records per SKU')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Promotion features\n",
    "featured_counts = train_data['is_featured_sku'].value_counts()\n",
    "axes[1, 0].bar(featured_counts.index, featured_counts.values)\n",
    "axes[1, 0].set_title('Featured SKU Distribution')\n",
    "axes[1, 0].set_xlabel('Is Featured (0/1)')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "display_counts = train_data['is_display_sku'].value_counts()\n",
    "axes[1, 1].bar(display_counts.index, display_counts.values)\n",
    "axes[1, 1].set_title('Display SKU Distribution')\n",
    "axes[1, 1].set_xlabel('Is Displayed (0/1)')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. 异常值检测与清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers in units_sold using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "outliers, lower_bound, upper_bound = detect_outliers_iqr(train_data, 'units_sold')\n",
    "\n",
    "print(f\"Lower bound for outliers: {lower_bound:.2f}\")\n",
    "print(f\"Upper bound for outliers: {upper_bound:.2f}\")\n",
    "print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "print(f\"Percentage of outliers: {len(outliers)/len(train_data)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_data['units_sold'], bins=100, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(lower_bound, color='r', linestyle='--', label=f'Lower bound ({lower_bound:.2f})')\n",
    "plt.axvline(upper_bound, color='r', linestyle='--', label=f'Upper bound ({upper_bound:.2f})')\n",
    "plt.title('Units Sold Distribution with Outlier Bounds')\n",
    "plt.xlabel('Units Sold')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(train_data['units_sold'])\n",
    "plt.title('Boxplot of Units Sold')\n",
    "plt.ylabel('Units Sold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outliers in price variables\n",
    "price_outliers_total, _, _ = detect_outliers_iqr(train_data, 'total_price')\n",
    "price_outliers_base, _, _ = detect_outliers_iqr(train_data, 'base_price')\n",
    "\n",
    "print(f\"Outliers in total_price: {len(price_outliers_total)} ({len(price_outliers_total)/len(train_data)*100:.2f}%)\")\n",
    "print(f\"Outliers in base_price: {len(price_outliers_base)} ({len(price_outliers_base)/len(train_data)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "# Check the specific missing value in total_price\n",
    "missing_price_rows = train_data[train_data['total_price'].isnull()]\n",
    "print(\"Rows with missing total_price in training data:\")\n",
    "display(missing_price_rows)\n",
    "\n",
    "# Impute missing value with base_price (reasonable assumption when no discount)\n",
    "train_data['total_price'].fillna(train_data['base_price'], inplace=True)\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates_train = train_data.duplicated().sum()\n",
    "duplicates_test = test_data.duplicated().sum()\n",
    "\n",
    "print(f\"Duplicate rows in training data: {duplicates_train}\")\n",
    "print(f\"Duplicate rows in test data: {duplicates_test}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicates_train > 0:\n",
    "    train_data = train_data.drop_duplicates()\n",
    "    print(f\"Removed {duplicates_train} duplicate rows from training data\")\n",
    "    \n",
    "if duplicates_test > 0:\n",
    "    test_data = test_data.drop_duplicates()\n",
    "    print(f\"Removed {duplicates_test} duplicate rows from test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type optimization\n",
    "def optimize_dtypes(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'int64':\n",
    "            if df[col].min() >= 0 and df[col].max() <= 255:\n",
    "                df[col] = df[col].astype('uint8')\n",
    "            elif df[col].min() >= -128 and df[col].max() <= 127:\n",
    "                df[col] = df[col].astype('int8')\n",
    "            elif df[col].min() >= 0 and df[col].max() <= 65535:\n",
    "                df[col] = df[col].astype('uint16')\n",
    "            elif df[col].min() >= -32768 and df[col].max() <= 32767:\n",
    "                df[col] = df[col].astype('int16')\n",
    "        elif df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "    return df\n",
    "\n",
    "# Optimize data types\n",
    "initial_memory_train = train_data.memory_usage(deep=True).sum() / 1024**2\n",
    "initial_memory_test = test_data.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "train_data = optimize_dtypes(train_data)\n",
    "test_data = optimize_dtypes(test_data)\n",
    "\n",
    "final_memory_train = train_data.memory_usage(deep=True).sum() / 1024**2\n",
    "final_memory_test = test_data.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "print(f\"Training data memory reduced from {initial_memory_train:.2f} MB to {final_memory_train:.2f} MB ({(initial_memory_train-final_memory_train)/initial_memory_train*100:.1f}% reduction)\")\n",
    "print(f\"Test data memory reduced from {initial_memory_test:.2f} MB to {final_memory_test:.2f} MB ({(initial_memory_test-final_memory_test)/initial_memory_test*100:.1f}% reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 5. 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-based features\n",
    "def create_time_features(df):\n",
    "    df = df.copy()\n",
    "    df['year'] = df['week'].dt.year\n",
    "    df['month'] = df['week'].dt.month\n",
    "    df['day_of_week'] = df['week'].dt.dayofweek\n",
    "    df['day_of_month'] = df['week'].dt.day\n",
    "    df['quarter'] = df['week'].dt.quarter\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    return df\n",
    "\n",
    "train_data = create_time_features(train_data)\n",
    "test_data = create_time_features(test_data)\n",
    "\n",
    "print(\"Time-based features created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create price-based features\n",
    "def create_price_features(df):\n",
    "    df = df.copy()\n",
    "    # Price difference and discount ratio\n",
    "    df['price_discount'] = df['base_price'] - df['total_price']\n",
    "    df['discount_ratio'] = df['price_discount'] / (df['base_price'] + 1e-8)  # Adding small epsilon to avoid division by zero\n",
    "    \n",
    "    # Log transformations\n",
    "    df['log_total_price'] = np.log1p(df['total_price'])\n",
    "    df['log_base_price'] = np.log1p(df['base_price'])\n",
    "    \n",
    "    # Price ratios\n",
    "    df['price_ratio'] = df['total_price'] / (df['base_price'] + 1e-8)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_data = create_price_features(train_data)\n",
    "test_data = create_price_features(test_data)\n",
    "\n",
    "print(\"Price-based features created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create promotion features\n",
    "def create_promotion_features(df):\n",
    "    df = df.copy()\n",
    "    # Combined promotion flag\n",
    "    df['is_promoted'] = ((df['is_featured_sku'] == 1) | (df['is_display_sku'] == 1)).astype(int)\n",
    "    \n",
    "    # Promotion intensity\n",
    "    df['promotion_intensity'] = df['is_featured_sku'] + df['is_display_sku']\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_data = create_promotion_features(train_data)\n",
    "test_data = create_promotion_features(test_data)\n",
    "\n",
    "print(\"Promotion features created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加滞后特征：过去1周、2周、4周的销量数据（仅用于训练数据）\n",
    "def create_lag_features_train(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 检查是否包含units_sold列\n",
    "    if 'units_sold' not in df.columns:\n",
    "        print(\"警告：数据中不包含units_sold列，无法创建滞后特征\")\n",
    "        return df\n",
    "    \n",
    "    # 创建一个用于计算滞后特征的数据框\n",
    "    # 需要按store_id和sku_id分组来计算滞后特征\n",
    "    lag_df = df[['week', 'store_id', 'sku_id', 'units_sold']].copy()\n",
    "    \n",
    "    # 按store_id和sku_id分组，按日期排序\n",
    "    lag_df = lag_df.sort_values(['store_id', 'sku_id', 'week'])\n",
    "    \n",
    "    # 计算滞后特征（1周、2周、4周前的销量）\n",
    "    lag_df['lag_1w'] = lag_df.groupby(['store_id', 'sku_id'])['units_sold'].shift(1)\n",
    "    lag_df['lag_2w'] = lag_df.groupby(['store_id', 'sku_id'])['units_sold'].shift(2)\n",
    "    lag_df['lag_4w'] = lag_df.groupby(['store_id', 'sku_id'])['units_sold'].shift(4)\n",
    "    \n",
    "    # 将滞后特征合并回原数据框\n",
    "    df = df.merge(lag_df[['week', 'store_id', 'sku_id', 'lag_1w', 'lag_2w', 'lag_4w']], \n",
    "                  on=['week', 'store_id', 'sku_id'], how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 为测试数据创建不依赖于units_sold的滞后特征函数\n",
    "def create_lag_features_test(df):\n",
    "    df = df.copy()\n",
    "    print(\"测试数据不创建依赖于units_sold的滞后特征\")\n",
    "    return df\n",
    "\n",
    "# 应用滞后特征\n",
    "print(\"正在创建滞后特征...\")\n",
    "train_data = create_lag_features_train(train_data)\n",
    "test_data = create_lag_features_test(test_data)\n",
    "\n",
    "# 检查新特征\n",
    "print(\"滞后特征创建完成！\")\n",
    "print(\"新增的滞后特征列:\")\n",
    "new_cols = [col for col in train_data.columns if col.startswith('lag_')]\n",
    "print(new_cols)\n",
    "\n",
    "# 查看包含滞后特征的样本数据\n",
    "print(\"\\n包含滞后特征的样本数据:\")\n",
    "display(train_data[['week', 'store_id', 'sku_id', 'units_sold', 'lag_1w', 'lag_2w', 'lag_4w']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加滚动统计特征：计算过去一段时间的销量均值、标准差等统计量（仅用于训练数据）\n",
    "def create_rolling_features_train(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 检查是否包含units_sold列\n",
    "    if 'units_sold' not in df.columns:\n",
    "        print(\"警告：数据中不包含units_sold列，无法创建滚动统计特征\")\n",
    "        return df\n",
    "    \n",
    "    # 创建一个用于计算滚动统计特征的数据框\n",
    "    rolling_df = df[['week', 'store_id', 'sku_id', 'units_sold']].copy()\n",
    "    \n",
    "    # 按store_id和sku_id分组，按日期排序\n",
    "    rolling_df = rolling_df.sort_values(['store_id', 'sku_id', 'week'])\n",
    "    \n",
    "    # 计算滚动统计特征（过去4周的均值和标准差）\n",
    "    rolling_df['rolling_mean_4w'] = rolling_df.groupby(['store_id', 'sku_id'])['units_sold'].transform(\n",
    "        lambda x: x.rolling(window=4, min_periods=1).mean())\n",
    "    rolling_df['rolling_std_4w'] = rolling_df.groupby(['store_id', 'sku_id'])['units_sold'].transform(\n",
    "        lambda x: x.rolling(window=4, min_periods=1).std())\n",
    "    \n",
    "    # 计算过去8周的均值和标准差\n",
    "    rolling_df['rolling_mean_8w'] = rolling_df.groupby(['store_id', 'sku_id'])['units_sold'].transform(\n",
    "        lambda x: x.rolling(window=8, min_periods=1).mean())\n",
    "    rolling_df['rolling_std_8w'] = rolling_df.groupby(['store_id', 'sku_id'])['units_sold'].transform(\n",
    "        lambda x: x.rolling(window=8, min_periods=1).std())\n",
    "    \n",
    "    # 将滚动统计特征合并回原数据框\n",
    "    df = df.merge(rolling_df[['week', 'store_id', 'sku_id', 'rolling_mean_4w', 'rolling_std_4w', \n",
    "                              'rolling_mean_8w', 'rolling_std_8w']], \n",
    "                  on=['week', 'store_id', 'sku_id'], how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 为测试数据创建不依赖于units_sold的滚动统计特征函数\n",
    "def create_rolling_features_test(df):\n",
    "    df = df.copy()\n",
    "    print(\"测试数据不创建依赖于units_sold的滚动统计特征\")\n",
    "    return df\n",
    "\n",
    "# 应用滚动统计特征\n",
    "print(\"正在创建滚动统计特征...\")\n",
    "train_data = create_rolling_features_train(train_data)\n",
    "test_data = create_rolling_features_test(test_data)\n",
    "\n",
    "# 检查新特征\n",
    "print(\"滚动统计特征创建完成！\")\n",
    "print(\"新增的滚动统计特征列:\")\n",
    "new_cols = [col for col in train_data.columns if col.startswith('rolling_')]\n",
    "print(new_cols)\n",
    "\n",
    "# 查看包含滚动统计特征的样本数据\n",
    "print(\"\\n包含滚动统计特征的样本数据:\")\n",
    "display(train_data[['week', 'store_id', 'sku_id', 'units_sold', 'rolling_mean_4w', 'rolling_std_4w', \n",
    "                    'rolling_mean_8w', 'rolling_std_8w']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加同比环比特征：计算销量的增长率和变化趋势（仅用于训练数据）\n",
    "def create_trend_features_train(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 检查是否包含units_sold列\n",
    "    if 'units_sold' not in df.columns:\n",
    "        print(\"警告：数据中不包含units_sold列，无法创建趋势特征\")\n",
    "        return df\n",
    "    \n",
    "    # 创建一个用于计算趋势特征的数据框\n",
    "    trend_df = df[['week', 'store_id', 'sku_id', 'units_sold']].copy()\n",
    "    \n",
    "    # 按store_id和sku_id分组，按日期排序\n",
    "    trend_df = trend_df.sort_values(['store_id', 'sku_id', 'week'])\n",
    "    \n",
    "    # 计算环比增长率（与上周相比）\n",
    "    trend_df['units_sold_lag1'] = trend_df.groupby(['store_id', 'sku_id'])['units_sold'].shift(1)\n",
    "    trend_df['week-over-week_growth'] = (trend_df['units_sold'] - trend_df['units_sold_lag1']) / (trend_df['units_sold_lag1'] + 1e-8)\n",
    "    \n",
    "    # 计算销量变化趋势（使用线性回归斜率）\n",
    "    def calculate_trend(series):\n",
    "        if len(series) < 2:\n",
    "            return 0\n",
    "        x = np.arange(len(series))\n",
    "        slope = np.polyfit(x, series, 1)[0] if len(series) > 1 else 0\n",
    "        return slope\n",
    "    \n",
    "    trend_df['sales_trend_4w'] = trend_df.groupby(['store_id', 'sku_id'])['units_sold'].transform(\n",
    "        lambda x: x.rolling(window=4, min_periods=1).apply(calculate_trend, raw=True))\n",
    "    \n",
    "    trend_df['sales_trend_8w'] = trend_df.groupby(['store_id', 'sku_id'])['units_sold'].transform(\n",
    "        lambda x: x.rolling(window=8, min_periods=1).apply(calculate_trend, raw=True))\n",
    "    \n",
    "    # 将趋势特征合并回原数据框\n",
    "    df = df.merge(trend_df[['week', 'store_id', 'sku_id', 'week-over-week_growth', 'sales_trend_4w', 'sales_trend_8w']], \n",
    "                  on=['week', 'store_id', 'sku_id'], how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 为测试数据创建不依赖于units_sold的趋势特征函数\n",
    "def create_trend_features_test(df):\n",
    "    df = df.copy()\n",
    "    print(\"测试数据不创建依赖于units_sold的趋势特征\")\n",
    "    return df\n",
    "\n",
    "# 应用趋势特征\n",
    "print(\"正在创建同比环比特征...\")\n",
    "train_data = create_trend_features_train(train_data)\n",
    "test_data = create_trend_features_test(test_data)\n",
    "\n",
    "# 检查新特征\n",
    "print(\"同比环比特征创建完成！\")\n",
    "print(\"新增的趋势特征列:\")\n",
    "new_cols = [col for col in train_data.columns if 'growth' in col or 'trend' in col]\n",
    "print(new_cols)\n",
    "\n",
    "# 查看包含趋势特征的样本数据\n",
    "print(\"\\n包含趋势特征的样本数据:\")\n",
    "display(train_data[['week', 'store_id', 'sku_id', 'units_sold', 'week-over-week_growth', 'sales_trend_4w', 'sales_trend_8w']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加商品/店铺历史表现：添加商品和店铺的历史平均销量等特征\n",
    "def create_historical_features_train(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 检查是否包含units_sold列\n",
    "    if 'units_sold' not in df.columns:\n",
    "        print(\"警告：数据中不包含units_sold列，无法创建历史表现特征\")\n",
    "        return df\n",
    "    \n",
    "    # 计算每个商品(sku_id)的历史平均销量、最大销量、最小销量\n",
    "    sku_stats = df.groupby('sku_id')['units_sold'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    sku_stats.columns = ['sku_id', 'sku_avg_sales', 'sku_std_sales', 'sku_min_sales', 'sku_max_sales']\n",
    "    \n",
    "    # 计算每个店铺(store_id)的历史平均销量、最大销量、最小销量\n",
    "    store_stats = df.groupby('store_id')['units_sold'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    store_stats.columns = ['store_id', 'store_avg_sales', 'store_std_sales', 'store_min_sales', 'store_max_sales']\n",
    "    \n",
    "    # 将历史统计特征合并回原数据框\n",
    "    df = df.merge(sku_stats, on='sku_id', how='left')\n",
    "    df = df.merge(store_stats, on='store_id', how='left')\n",
    "    \n",
    "    # 计算商品和店铺的销量占比特征\n",
    "    df['sku_sales_ratio'] = df['units_sold'] / (df['sku_avg_sales'] + 1e-8)\n",
    "    df['store_sales_ratio'] = df['units_sold'] / (df['store_avg_sales'] + 1e-8)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_historical_features_test(df, train_df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 使用训练数据的统计信息\n",
    "    sku_stats = train_df.groupby('sku_id')['units_sold'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    sku_stats.columns = ['sku_id', 'sku_avg_sales', 'sku_std_sales', 'sku_min_sales', 'sku_max_sales']\n",
    "    \n",
    "    store_stats = train_df.groupby('store_id')['units_sold'].agg(['mean', 'std', 'min', 'max']).reset_index()\n",
    "    store_stats.columns = ['store_id', 'store_avg_sales', 'store_std_sales', 'store_min_sales', 'store_max_sales']\n",
    "    \n",
    "    # 将历史统计特征合并到测试数据\n",
    "    df = df.merge(sku_stats, on='sku_id', how='left')\n",
    "    df = df.merge(store_stats, on='store_id', how='left')\n",
    "    \n",
    "    # 计算商品和店铺的销量占比特征（使用历史平均值）\n",
    "    df['sku_sales_ratio'] = df['sku_avg_sales'] / (df['sku_avg_sales'] + 1e-8)  # 这里使用历史平均值\n",
    "    df['store_sales_ratio'] = df['store_avg_sales'] / (df['store_avg_sales'] + 1e-8)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 应用历史表现特征\n",
    "print(\"正在创建商品/店铺历史表现特征...\")\n",
    "train_data = create_historical_features_train(train_data)\n",
    "test_data = create_historical_features_test(test_data, train_data)\n",
    "\n",
    "# 检查新特征\n",
    "print(\"商品/店铺历史表现特征创建完成！\")\n",
    "print(\"新增的历史表现特征列:\")\n",
    "new_cols = [col for col in train_data.columns if 'sku_' in col or 'store_' in col]\n",
    "print(new_cols)\n",
    "\n",
    "# 查看包含历史表现特征的样本数据\n",
    "print(\"\\n包含历史表现特征的样本数据:\")\n",
    "display(train_data[['week', 'store_id', 'sku_id', 'units_sold', 'sku_avg_sales', 'store_avg_sales', \n",
    "                    'sku_sales_ratio', 'store_sales_ratio']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加交叉特征：创建促销与时间等因素的交叉特征\n",
    "def create_interaction_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 促销与时间的交叉特征\n",
    "    # 促销类型与星期的交叉\n",
    "    df['featured_weekday'] = df['is_featured_sku'] * df['day_of_week']\n",
    "    df['display_weekday'] = df['is_display_sku'] * df['day_of_week']\n",
    "    \n",
    "    # 促销组合与月份的交叉\n",
    "    df['promoted_month'] = df['is_promoted'] * df['month']\n",
    "    \n",
    "    # 价格折扣与促销的交叉\n",
    "    df['discount_featured'] = df['discount_ratio'] * df['is_featured_sku']\n",
    "    df['discount_display'] = df['discount_ratio'] * df['is_display_sku']\n",
    "    \n",
    "    # 价格与星期的交叉\n",
    "    df['price_weekday'] = df['total_price'] * df['day_of_week']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 应用交叉特征\n",
    "print(\"正在创建交叉特征...\")\n",
    "train_data = create_interaction_features(train_data)\n",
    "test_data = create_interaction_features(test_data)\n",
    "\n",
    "# 检查新特征\n",
    "print(\"交叉特征创建完成！\")\n",
    "print(\"新增的交叉特征列:\")\n",
    "interaction_cols = ['featured_weekday', 'display_weekday', 'promoted_month', 'discount_featured', \n",
    "                   'discount_display', 'price_weekday']\n",
    "print(interaction_cols)\n",
    "\n",
    "# 查看包含交叉特征的样本数据\n",
    "print(\"\\n包含交叉特征的样本数据:\")\n",
    "display(train_data[['week', 'store_id', 'sku_id', 'units_sold', 'is_featured_sku', 'is_display_sku',\n",
    "                    'day_of_week', 'month', 'discount_ratio', 'total_price'] + interaction_cols].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis of engineered features with target\n",
    "feature_cols = ['total_price', 'base_price', 'is_featured_sku', 'is_display_sku', 'price_discount', 'discount_ratio', 'log_total_price', 'log_base_price', 'is_promoted', 'promotion_intensity', 'month', 'day_of_week']\n",
    "\n",
    "correlation_data = train_data[feature_cols + ['units_sold']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_data, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Features with Target Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 6. 数据质量验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataset summary and data quality validation\n",
    "print(\"=== FINAL DATASET SUMMARY ===\")\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "# 显示所有特征列（不包括record_ID和目标变量）\n",
    "train_features = [col for col in train_data.columns if col not in ['record_ID', 'week', 'store_id', 'sku_id', 'units_sold']]\n",
    "test_features = [col for col in test_data.columns if col not in ['record_ID', 'week', 'store_id', 'sku_id']]\n",
    "print(f\"\\nTraining features: {len(train_features)}\")\n",
    "print(f\"Test features: {len(test_features)}\")\n",
    "\n",
    "# 检查特征一致性\n",
    "common_features = set(train_features) & set(test_features)\n",
    "train_only_features = set(train_features) - set(test_features)\n",
    "test_only_features = set(test_features) - set(train_features)\n",
    "print(f\"Common features: {len(common_features)}\")\n",
    "print(f\"Training-only features: {len(train_only_features)}\")\n",
    "print(f\"Test-only features: {len(test_only_features)}\")\n",
    "\n",
    "if train_only_features:\n",
    "    print(f\"\\nTraining-only features (should be mostly target-dependent): {train_only_features}\")\n",
    "    \n",
    "if test_only_features:\n",
    "    print(f\"\\nTest-only features: {test_only_features}\")\n",
    "\n",
    "# 检查缺失值\n",
    "print(\"\\n=== MISSING VALUES CHECK ===\")\n",
    "train_missing = train_data[train_features].isnull().sum()\n",
    "test_missing = test_data[test_features].isnull().sum()\n",
    "print(f\"Missing values in training data: {train_missing.sum()}\")\n",
    "print(f\"Missing values in test data: {test_missing.sum()}\")\n",
    "\n",
    "# 按类别分组显示特征\n",
    "time_features = [col for col in train_features if col in ['year', 'month', 'day_of_week', 'day_of_month', 'quarter', 'is_weekend']]\n",
    "price_features = [col for col in train_features if col in ['total_price', 'base_price', 'price_discount', 'discount_ratio', 'log_total_price', 'log_base_price', 'price_ratio']]\n",
    "promotion_features = [col for col in train_features if col in ['is_featured_sku', 'is_display_sku', 'is_promoted', 'promotion_intensity']]\n",
    "lag_features = [col for col in train_features if col.startswith('lag_')]\n",
    "rolling_features = [col for col in train_features if col.startswith('rolling_')]\n",
    "trend_features = [col for col in train_features if 'growth' in col or 'trend' in col]\n",
    "historical_features = [col for col in train_features if col.startswith('sku_') or col.startswith('store_')]\n",
    "interaction_features = [col for col in train_features if col in ['featured_weekday', 'display_weekday', 'promoted_month', 'discount_featured', 'discount_display', 'price_weekday']]\n",
    "\n",
    "print(\"\\n=== FEATURE CATEGORIES ===\")\n",
    "print(f\"Time-based features ({len(time_features)}): {time_features}\")\n",
    "print(f\"Price-based features ({len(price_features)}): {price_features}\")\n",
    "print(f\"Promotion features ({len(promotion_features)}): {promotion_features}\")\n",
    "print(f\"Lag features ({len(lag_features)}): {lag_features}\")\n",
    "print(f\"Rolling statistics features ({len(rolling_features)}): {rolling_features}\")\n",
    "print(f\"Trend features ({len(trend_features)}): {trend_features}\")\n",
    "print(f\"Historical performance features ({len(historical_features)}): {historical_features}\")\n",
    "print(f\"Interaction features ({len(interaction_features)}): {interaction_features}\")\n",
    "\n",
    "# 保存处理后的数据（移除record_ID列）\n",
    "train_processed = train_data.drop(['record_ID'], axis=1)\n",
    "test_processed = test_data.drop(['record_ID'], axis=1)\n",
    "\n",
    "train_processed.to_csv('../archive/train_processed_improved.csv', index=False)\n",
    "test_processed.to_csv('../archive/test_processed_improved.csv', index=False)\n",
    "print(\"\\nProcessed data saved to archive folder.\")\n",
    "print(\"File names: train_processed_improved.csv, test_processed_improved.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "完成了改进版的数据分析pipeline，并实现了高级特征工程：\n",
    "\n",
    "1. **数据加载**：加载训练和测试数据集\n",
    "2. **探索性数据分析**：分析分布、相关性和模式\n",
    "3. **异常值检测**：使用IQR方法识别异常值\n",
    "4. **数据预处理**：处理缺失值、删除重复项、优化数据类型\n",
    "5. **基础特征工程**：创建了基于时间、基于价格、基于促销的特征\n",
    "6. **高级特征工程**：\n",
    "   - 滞后特征：过去1周、2周、4周的销量数据（仅用于训练数据）\n",
    "   - 滚动统计特征：过去一段时间的销量均值、标准差等统计量（仅用于训练数据）\n",
    "   - 同比环比特征：销量的增长率和变化趋势（仅用于训练数据）\n",
    "   - 商品/店铺历史表现：商品和店铺的历史平均销量等特征\n",
    "   - 交叉特征：促销与时间等因素的交叉特征\n",
    "7. **数据质量验证**：检查特征一致性、缺失值等\n",
    "\n",
    "### 改进点\n",
    "1. **数据一致性**：确保训练集和测试集具有相同的特征列（除了目标变量相关的特征）\n",
    "2. **时间序列处理**：正确处理时间序列特征，避免数据泄露\n",
    "3. **错误处理**：添加了适当的错误检查和处理机制\n",
    "4. **数据验证**：添加了数据质量验证步骤\n",
    "5. **特征清理**：移除了无用的record_ID列\n",
    "\n",
    "接下来可以进行机器学习建模..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}